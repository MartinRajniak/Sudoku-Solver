# We need to limit data to be able to run faster experiments
DATA_SIZE_LIMIT = 10_000

BATCH_SIZE = 256 # Best accurracy & time for current learning rate
LEARNING_RATE = 1e-3
EPOCHS = 3

# Use residual connections in convolutional model architecture
USE_RESIDUAL = true

# Idea is to pretrain model on solution-only data (self-supervised learning) like autoencoder
#
# Note: There is no performance improvement on small data sample (10%) and few epochs training (1)
# Turn on when training with more data and epochs to see if it helps
USE_PRE_TRAINING = false

# Disk cache is slower but a must if we can't load everything into a memory
USE_DISK_CACHE = false

# Idea is to let model learn first on easy problems and then gradually increase problem difficulty
USE_CURRICULUM_LEARNING = true

# Idea is to create a layer that would penalise predictions that break Sudoku rules
CONSTRAINT_WEIGHT = 0.1

# Fixed cell penalty only makes sense if we increase weight against cross-entropy,
# otherwise we could just use cross-entropy to check fixed number predictions
FIXED_CELL_WEIGHT = 10

USE_WEIGHT_SCHEDULING = True

# Replace predictions for fixed numbers with actuall fixed numbers
#
# Idea is to force model to ignore fixed numbers
#
# Note: so far it seems that model is unable to learn with this turned on
USE_FIXED_NUMBER_LAYER = false

# Mix train datasets so that in each dataset, there is PRIMARY_DATASET_SPLIT of primary difficulty 
# and 1 - PRIMARY_DATASET_SPLIT of already seen difficulties
#
# Note: At least for initial training, mixing datasets is detrimental - slower convergence and lower accuracy
USE_DATASET_MIXING = false
PRIMARY_DATASET_SPLIT = 0.9